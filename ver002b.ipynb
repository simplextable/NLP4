{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 5000000)\n",
    "pd.set_option('display.max_columns', 5000000)\n",
    "pd.set_option('display.width', 10000000)\n",
    "pd.set_option('display.max_colwidth', 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample = pd.read_csv(\"sample.csv\")\n",
    "\n",
    "\n",
    "df=pd.concat([train,test])\n",
    "target = train[[\"label\"]]\n",
    "df.drop([\"label\", \"id\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ ile başlayan kelimelerin hepsini dopladık\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt    \n",
    "\n",
    "df['tweet'] = np.vectorize(remove_pattern)(df['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kısaltma Düzelme\n",
    "\"\"\"\"\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"n\\’t\", \" not\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"\\’d\", \" would\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "def clean(tweet):\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
    "    return tweet    \n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda s : clean(s))    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    \n",
    "    return url.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet\"]=df[\"tweet\"].str.replace(\"[^A-Za-z]\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tek Harfliler droplanması\n",
    "\n",
    "df[\"tweet\"] = df[\"tweet\"].str.replace(r\"\\b[a-zA-Z]\\b\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "#nltk.download('wordnet')\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil = pd.Series(' '.join(df['tweet']).split()).value_counts()[-39000:]\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in sil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manuel Kelimeler\n",
    "\"\"\"\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"amp\", \"\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"hu\", \"\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"feminismmuktbharat\", \"feminis\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"feminismiscancer\", \"feminis\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"feminismisterrorism\", \"feminis\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihday\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdaygirl\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdays\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdaysway\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdaygs\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdaytome\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdayboy\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdaypresent\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdaykazuki\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdayjodistamaria\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"itsmybihday\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"thbihday\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"bihdaycake\", \"birthday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"trsdaythoughts\", \"trsday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"throwbacktrsday\", \"trsday\", str(x)))\n",
    "df['tweet'] = df['tweet'].map(lambda x: re.sub(r\"thankfultrsday\", \"trsday\", str(x)))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1e58c63eab49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mtrain_glove_oov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_glove_vocab_coverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_glove_text_coverage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_embeddings_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglove_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_glove_vocab_coverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_glove_text_coverage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "#glove_embeddings = np.load('C:\\\\Users\\\\MONSTER\\Documents\\\\PY\\\\NLP\\Anality-Data-Twitter-Sentiment\\\\glove.840B.300d.pkl', allow_pickle=True)\n",
    "\n",
    "\n",
    "def build_vocab(X):\n",
    "    \n",
    "    tweets = X.apply(lambda s: s.split()).values      \n",
    "    vocab = {}\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1                \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_embeddings_coverage(X, embeddings):\n",
    "    \n",
    "    vocab = build_vocab(X)    \n",
    "    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage\n",
    "\n",
    "\n",
    "\n",
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df['tweet'], glove_embeddings)\n",
    "\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISASTER_TWEETS = train['label'] == 1\n",
    "\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' if token not in sw]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "N = 100\n",
    "\n",
    "# Unigrams\n",
    "disaster_unigrams = defaultdict(int)\n",
    "nondisaster_unigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[DISASTER_TWEETS]['tweet']:\n",
    "    for word in generate_ngrams(tweet):\n",
    "        disaster_unigrams[word] += 1\n",
    "        \n",
    "for tweet in df[~DISASTER_TWEETS]['tweet']:\n",
    "    for word in generate_ngrams(tweet):\n",
    "        nondisaster_unigrams[word] += 1\n",
    "        \n",
    "df_disaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_nondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "# Bigrams\n",
    "disaster_bigrams = defaultdict(int)\n",
    "nondisaster_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[DISASTER_TWEETS]['tweet']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        disaster_bigrams[word] += 1\n",
    "        \n",
    "for tweet in df[~DISASTER_TWEETS]['tweet']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        nondisaster_bigrams[word] += 1\n",
    "        \n",
    "df_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "# Trigrams\n",
    "disaster_trigrams = defaultdict(int)\n",
    "nondisaster_trigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[DISASTER_TWEETS]['tweet']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        disaster_trigrams[word] += 1\n",
    "        \n",
    "for tweet in df[~DISASTER_TWEETS]['tweet']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        nondisaster_trigrams[word] += 1\n",
    "        \n",
    "df_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='red')\n",
    "sns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='green')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common unigrams in Disaster Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common unigrams in Non-disaster Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='red')\n",
    "sns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='green')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common bigrams in Disaster Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common bigrams in Non-disaster Tweets', fontsize=15)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(20, 50), dpi=100)\n",
    "\n",
    "sns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='red')\n",
    "sns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='green')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=11)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common trigrams in Disaster Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common trigrams in Non-disaster Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df[:train.shape[0]]\n",
    "test=df[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.3, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 18467)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 4), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = 'english')\n",
    "\n",
    "X_train_counts = vec.fit_transform(train['tweet'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9589, 18467)\n"
     ]
    }
   ],
   "source": [
    "X_val_counts = vec.transform(x_val['tweet'])\n",
    "X_val_tfidf = tfidf_transformer.transform(X_val_counts)\n",
    "print(X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17197, 18467)\n"
     ]
    }
   ],
   "source": [
    "X_test_counts = vec.transform(test['tweet'])\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_log = LogisticRegression(random_state=2019, C=10000).fit(X_train_tfidf, target)\n",
    "y_pred = clf_log.predict(X_test_tfidf)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for C=10: 0.9121212121212121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for C=50: 0.9865343727852587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for C=100: 0.9908256880733946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for C=1000: 0.9943741209563995\n",
      "f1 score for C=10000: 0.9964862965565706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "for c in [10, 50, 100, 1000, 10000]:\n",
    "    lr = LogisticRegression(C=c, random_state=2019).fit(X_train_tfidf, target)\n",
    "    #lr = LogisticRegression(C=c, random_state=2019).fit(X_train_tfidf, y_train)\n",
    "    print (\"f1 score for C=%s: %s\" % (c, f1_score(y_val, lr.predict(X_val_tfidf))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9981023428713577\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_val, clf_log.predict(X_val_tfidf), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonuc = pd.DataFrame(data = y_pred, index = range(17197), columns=[\"label\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.read_csv('sample.csv')\n",
    "id = submission[[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2= pd.concat([id,sonuc],axis=1)\n",
    "s2.to_csv('230620_5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
