{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "target = pd.read_csv(\"train.csv\")\n",
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "\n",
    "df=pd.concat([train,test])\n",
    "Y = train[[\"label\"]]\n",
    "df.drop([\"label\", \"id\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ ile başlayan kelimelerin hepsini dopladık\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt    \n",
    "\n",
    "df['tweet'] = np.vectorize(remove_pattern)(df['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    \n",
    "    return url.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet\"]=df[\"tweet\"].str.replace(\"[^A-Za-z]\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet\"] = df[\"tweet\"].str.replace(r\"\\b[a-zA-Z]\\b\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "#nltk.download('wordnet')\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil = pd.Series(' '.join(df['tweet']).split()).value_counts()[-39000:]\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in sil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df['tweet'].values)\n",
    "X = tokenizer.texts_to_sequences(df['tweet'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[:train.shape[0]]\n",
    "test=X[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25569, 17) (25569, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 17, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 17, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 197       \n",
      "=================================================================\n",
      "Total params: 510,997\n",
      "Trainable params: 510,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "25569/25569 [==============================] - 15s 569us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 2/15\n",
      "25569/25569 [==============================] - 13s 489us/step - loss: 14.2622 - accuracy: 0.06994s - loss: 14.232\n",
      "Epoch 3/15\n",
      "25569/25569 [==============================] - 12s 479us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 4/15\n",
      "25569/25569 [==============================] - 12s 474us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 5/15\n",
      "25569/25569 [==============================] - 12s 481us/step - loss: 14.2622 - accuracy: 0.06994s - \n",
      "Epoch 6/15\n",
      "25569/25569 [==============================] - 12s 473us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 7/15\n",
      "25569/25569 [==============================] - 13s 496us/step - loss: 14.2622 - accuracy: 0.06990s - loss: 14.2648 - accuracy: 0.069\n",
      "Epoch 8/15\n",
      "25569/25569 [==============================] - 13s 492us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 9/15\n",
      "25569/25569 [==============================] - 13s 492us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 10/15\n",
      "25569/25569 [==============================] - 13s 490us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 11/15\n",
      "25569/25569 [==============================] - 13s 492us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 12/15\n",
      "25569/25569 [==============================] - ETA: 0s - loss: 14.2623 - accuracy: 0.069 - 12s 489us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 13/15\n",
      "25569/25569 [==============================] - 13s 490us/step - loss: 14.2622 - accuracy: 0.0699\n",
      "Epoch 14/15\n",
      "25569/25569 [==============================] - 13s 493us/step - loss: 14.2622 - accuracy: 0.06991s - loss: 14.2610 -\n",
      "Epoch 15/15\n",
      "25569/25569 [==============================] - 13s 490us/step - loss: 14.2622 - accuracy: 0.0699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c63ae5be88>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
